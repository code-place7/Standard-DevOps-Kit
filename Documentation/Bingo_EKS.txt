ğŸš€ REAL WORLD DEVOPS PROJECT â€” BINGO
End-to-End CI/CD + EKS + Monitoring + Terraform

============================
ğŸ§  UNDERSTANDING JIRA

JIRA is a project management tool.

Team Lead assigns tasks â†’ based on your role.

Task flow:

TO DO â†’ IN PROGRESS â†’ DONE

When you drag a task to IN PROGRESS, your lead gets notified.
Simple workflow, but very important in real-world teams.

============================
ğŸ¯ PROJECT GOAL

What I built:

Bingo app deployed on Kubernetes with CI/CD

SonarQube, Trivy, OWASP integrated into pipeline

Email alerts with scan reports

Secure subdomain with SSL (Cloudflare)

Monitoring using Prometheus + Grafana

HTTPS traffic tracking

Infrastructure created using Terraform

============================
ğŸ—ï¸ STEP 1 â€” INFRASTRUCTURE

Created Ubuntu EC2 instance in AWS

Configured Security Groups

SSH into VM

Ran:

sudo apt update

Opened Ports:

8080 â†’ Jenkins
3000 â†’ Application
9000 â†’ SonarQube

============================
ğŸ› ï¸ TOOLS INSTALLED

Jenkins

Docker

SonarQube

AWS CLI

kubectl

eksctl

Terraform

Trivy

Note: Manual installation takes time.
Better approach â†’ create installation scripts.

============================
ğŸ”§ JENKINS SETUP

Access Jenkins:

http://yourEC2IP:8080

Installed Plugins:

Eclipse Temurin Installer

SonarQube Scanner

NodeJS

OWASP Dependency Check

Pipeline Stage View

Docker Plugins

============================
ğŸ§° TOOL CONFIGURATION

Configured tools with exact names:

JDK â†’ jdk17
Sonar Scanner â†’ sonar-scanner
NodeJS â†’ node16
Dependency Check â†’ DP
Docker â†’ docker

Important:
The name here MUST match what you call inside Jenkinsfile.
Wrong name = pipeline failure.

============================
ğŸ” SONARQUBE INTEGRATION

SonarQube â†’ Admin â†’ Security â†’ Users â†’ Generate Token

Token Name: jenkin

Saved in Jenkins Credentials as:

ID: Sonar-token
Type: Secret Text

Configured Sonar server inside Jenkins System Configuration.

============================
ğŸ³ DOCKER CREDENTIALS

Stored DockerHub username & password in Jenkins credentials.
Used later for pushing image to DockerHub.

============================
ğŸ” BASIC PIPELINE

First pipeline stage:

Git Checkout

After build â†’ repo visible inside Workspace tab.

Then I realized:
â€œThis is just fetching code. Now we automate everything.â€

============================
ğŸ”¥ FULL CI/CD PIPELINE

Stages added:

Clean Workspace

Checkout

SonarQube Analysis

Quality Gate

npm install

OWASP Scan

Trivy FS Scan

Docker Build

Docker Push

Trivy Image Scan

Deploy Container

After build:

Sonar showed vulnerabilities

Trivy generated txt files

Docker image created

Container running on port 3000

Commands used for verification:

docker images
docker ps -a
docker logs -f <container_id>

Access app:

http://EC2IP:3000

============================
ğŸ“§ EMAIL NOTIFICATIONS

Problem:
Scan reports were only inside Jenkins.

Solution:
Configured Gmail SMTP using App Password.

SMTP Details:

smtp.gmail.com
Port 587 / 465
TLS + SSL

Installed Email Extension plugin.
Updated pipeline with post block to send reports.

Now every build sends:

Build status

Build number

Build URL

Trivy reports as attachments

Now it feels real production-ready.

============================
â˜¸ï¸ MOVING TO EKS

eksctl â†’ Creates cluster
kubectl â†’ Manages apps inside cluster

Earlier: only AWS dashboard clicks.
Now: CLI usage.

So I needed IAM user.

============================
ğŸ”‘ IAM + AWS CLI

Created IAM user: bingok8s

Attached policies:

EC2 Full Access

VPC Full Access

Admin Access

CloudFormation Full Access

IAM Full Access

Generated Access Key.

Configured in VM:

aws configure

============================
ğŸš€ CREATE EKS CLUSTER

Command:

eksctl create cluster --name your_cluster_name
--region ap-south-1
--node-type t2.medium
--zones ap-south-1a,ap-south-1b

Verified in AWS Dashboard:

Cluster created

2 nodes

Different availability zones

============================
ğŸ“„ KUBERNETES MANIFEST

Why?

Instead of single container:

Replicas

Auto scaling

Load balancing

Self-healing

Applied using:

kubectl apply -f manifest.yml

Verification:

kubectl get pods
kubectl logs <pod-name>

Access via Load Balancer DNS.

Traffic distribution handled by:

Kubernetes Service

kube-proxy

Scheduler decides pod placement

============================
ğŸŒ DOMAIN + SSL

Used Cloudflare:

Added domain

Mapped Load Balancer

Enabled SSL

Now app looks production-ready.

============================
ğŸ—ï¸ TERRAFORM (MONITORING EC2)

Files:

main.tf

variables.tf

output.tf

Commands:

terraform init
terraform plan
terraform apply

Verified EC2 creation in AWS.

============================
ğŸ¤¯ CONFUSION MOMENT

I thought:

â€œWhy is my repo not in this VM?â€

Then realized:

Earlier deployment was done through Jenkins pipeline.

Now using CLI â†’ I must clone repo manually.

mkdir bingo
cd bingo
git clone <repo>

Everything became clear.

============================
ğŸ“Š MONITORING SETUP

On Monitoring EC2:

sudo su
sudo apt update

Installed Prometheus.

Used clean structure:

mkdir -p /etc/prometheus

Checked port:

netstat -tulnp

Prometheus â†’ Port 9090

Initially not accessible â†’ Security group missing.
Fixed inbound rule.

============================
ğŸ›°ï¸ BLACKBOX EXPORTER

Tracks:

HTTP

HTTPS

DNS

TCP

ICMP

Started:

./blackbox_exporter &

Runs on port 9115.

Configured Prometheus.yml with targets.

Restarted Prometheus.

Checked Target Health in dashboard.

============================
ğŸ“ˆ GRAFANA

Started service:

sudo systemctl start grafana-server

Opened security group port.

Inside Grafana:

Added Prometheus as Data Source

Imported Blackbox dashboard

Now metrics visualized properly.

============================
ğŸ–¥ï¸ NODE EXPORTER

Important:

Node Exporter â†’ Installed on MAIN EC2
Prometheus config â†’ Edited on MONITORING EC2

Started:

./node_exporter &

Port: 9100

Updated Prometheus.yml.
Restarted Prometheus in background:

./prometheus > prometheus.log 2>&1 &

Imported Node Exporter dashboard in Grafana.

Now visible:

CPU

RAM

Memory

System Load

Architecture connected:

Blackbox â†’ External traffic
Node Exp â†’ System health
Prometheus â†’ Collector
Grafana â†’ Visualization

============================
ğŸ§¹ CLEANUP

After practice:

terraform destroy

Also manually checked AWS dashboard
Deleted any manually created resources.

Clean account = good DevOps habit.

============================
ğŸ WHAT I LEARNED

Not about memorizing commands.

It was about understanding:

CI/CD automation

Infrastructure as Code

Kubernetes scaling

Monitoring architecture

CLI vs Dashboard usage

Security best practices

Production mindset

And honestlyâ€¦

The confusion moments taught me more than the smooth ones.