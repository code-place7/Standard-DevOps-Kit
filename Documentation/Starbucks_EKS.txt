ğŸš€ REAL WORLD DEVOPS PROJECT â€“ 1
â˜• Starbucks App | CI/CD â†’ EKS â†’ Monitoring

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ PROJECT GOAL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Build a real production-style DevOps setup where:

â€¢ Code builds automatically
â€¢ Quality scan runs
â€¢ Security scan runs
â€¢ Docker image pushed
â€¢ App deployed to Kubernetes
â€¢ Domain + SSL configured
â€¢ Monitoring added
â€¢ Infrastructure automated with Terraform

Not just â€œit runsâ€, but full lifecycle.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ–¥ï¸ PHASE 1 â€“ JENKINS SERVER SETUP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Launched Ubuntu EC2.

Attached Elastic IP (so public IP doesnâ€™t change).

Configured Security Group:

22 â†’ SSH
8080 â†’ Jenkins
3000 â†’ App
80 / 443 â†’ Web
587 â†’ Email

Updated system and installed Jenkins using script.

Accessed Jenkins via browser on port 8080.

Installed required plugins:

â€¢ Docker
â€¢ NodeJS
â€¢ SonarQube Scanner
â€¢ Pipeline Stage View
â€¢ Email Extension

Now CI server ready.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‚ PHASE 2 â€“ PROJECT STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Cloned full GitHub repo.

Repository contained:

â€¢ App code
â€¢ Jenkinsfile
â€¢ Dockerfile
â€¢ Kubernetes manifests
â€¢ Terraform files
â€¢ Monitoring configs
â€¢ Installation scripts

Made scripts executable and ran them.

Lesson learned:
Automation > Manual setup.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” PHASE 3 â€“ SONARQUBE SETUP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Launched separate EC2 for SonarQube.

Ran SonarQube container on port 9000.

Generated authentication token.

Connected SonarQube to Jenkins using:

â€¢ Token in credentials
â€¢ Webhook for Quality Gate

Now pipeline included:

Code Analysis â†’ Quality Gate â†’ Continue if passed.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ PHASE 4 â€“ DOCKER BASED CI/CD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Pipeline stages were:

â€¢ Clean workspace
â€¢ Checkout code
â€¢ SonarQube scan
â€¢ npm install
â€¢ Trivy filesystem scan
â€¢ Docker build
â€¢ Docker push
â€¢ Trivy image scan
â€¢ Deploy container

App was running on EC2 at port 3000.

Everything worked.

Butâ€¦

Single container is not scalable.

If EC2 dies â†’ app dies.

Thatâ€™s when I decided to move to Kubernetes.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â˜¸ï¸ PHASE 5 â€“ EKS SETUP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Created IAM user for automation.

Configured AWS CLI.

Used eksctl to:

â€¢ Create EKS cluster
â€¢ Create nodegroup
â€¢ Associate OIDC
â€¢ Update kubeconfig

Verified nodes using kubectl.

Important mistake I made:

AWS CLI was configured for ubuntu user.
But Jenkins runs as jenkins user.

So Jenkins couldnâ€™t access AWS.

Fixed it by switching to jenkins user and configuring AWS there.

Big learning moment.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” PHASE 6 â€“ PIPELINE MODIFIED FOR EKS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Removed container-based deploy.

Added Kubernetes deployment stage.

Pipeline now:

Build â†’ Push â†’ Update kubeconfig â†’ Apply manifest â†’ Verify pods & services.

Manifest included:

Deployment with 2 replicas.
Service type LoadBalancer.

AWS automatically created Load Balancer.

Got external URL.

Now app is scalable.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸŒ PHASE 7 â€“ DOMAIN & SSL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Used Cloudflare.

Mapped domain to LoadBalancer.

Enabled SSL.

Now app accessible over HTTPS.

More production-ready.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š PHASE 8 â€“ MONITORING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Created separate Monitoring EC2 using Terraform.

Installed:

â€¢ Prometheus
â€¢ Grafana
â€¢ Blackbox Exporter

Configured Prometheus targets.

Connected Grafana to Prometheus.

Imported dashboards.

Now monitoring:

â€¢ Application availability
â€¢ HTTP metrics
â€¢ Endpoint status

Mistake I made:

Forgot to open Blackbox port in Security Group.

Prometheus target was down.

Fixed Security Group â†’ restarted service â†’ working.

Lesson:
Security Groups are always part of debugging.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ—ï¸ FINAL ARCHITECTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Jenkins â†’ CI/CD automation
SonarQube â†’ Code quality
Trivy â†’ Security scanning
DockerHub â†’ Image storage
EKS â†’ Container orchestration
Cloudflare â†’ DNS + SSL
Terraform â†’ Infrastructure as Code
Prometheus â†’ Metrics collection
Grafana â†’ Visualization
Blackbox â†’ Uptime monitoring

Complete DevOps lifecycle.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  WHAT I LEARNED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â€¢ IAM matters for automation
â€¢ Jenkins user vs Ubuntu user difference
â€¢ Security Groups cause most issues
â€¢ Kubernetes solves scaling
â€¢ Monitoring completes DevOps
â€¢ Automation reduces human error

Most importantly:

The confusion moments taught me more than the successful ones.