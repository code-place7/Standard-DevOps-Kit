============================================================ 
DEVOPSJOURNEY â€“ MISTAKES & LESSONS LEARNED (February Real-World PracticeProjects) 
============================================================

1. The â€œWhy is it not opening in browser?â€ mistake ðŸ˜…

I installed Jenkins. I installed SonarQube. Everything was running
perfectly.

But nothing was opening in the browser.

After 10 minutes of confusionâ€¦ I realized I forgot to open the required
ports in the Security Group.

Lesson: If a service is running but not accessible in the browser,
ALWAYS check Security Groups first.

2. The â€œWhere is my repo?â€ confusion ðŸ¤¦

I was inside the VM thinking: â€œWhere is my project code?â€

Then I remembered â€” Jenkins cloned the repo inside its own workspace
during pipeline execution.

The repo was never cloned manually inside my EC2 VM.

Lesson: Jenkins workspace â‰  EC2 filesystem. If working from CLI, clone
the repository manually.

3. The â€œWaitâ€¦ we never used AWS CLI before?â€ realization ðŸ¤¯
------------------------------------------------------------

Till now, I was deploying my app using Jenkins dashboard.

I wrote the pipeline inside Jenkins.
Jenkins pulled code from GitHub.
Built Docker image.
Ran container.
Done.

Everything was happening through dashboard and automation.
I never needed AWS CLI.
I never needed IAM user.
I was just clicking in AWS dashboard when needed.

But nowâ€¦

I wanted to create an EKS cluster using:

eksctl create cluster

And that command needs AWS CLI authentication.

Thatâ€™s when I realized:

We never created an IAM user for CLI access.
We were just using dashboard clicks till now.

So first step:
Go to IAM â†’ Create a new user.

I created a user named something like:
bingok8s

Then attached required policies:
- EC2 Full Access
- VPC Full Access
- CloudFormation Full Access
- IAM Full Access
- Administrator Access (for learning phase)

Then created an Access Key.

After that, inside the VM:

aws configure

Entered:
Access Key
Secret Key
Region (ap-south-1)

Now finally my CLI was authenticated with AWS.

Then I ran:

eksctl create cluster --name your_cluster_name \
--region ap-south-1 \
--node-type t2.medium \
--zones ap-south-1a,ap-south-1b

After some time, cluster was created.

When I checked AWS dashboard:
âœ” EKS cluster created
âœ” Two nodes running
âœ” Same instance type (t2.medium)
âœ” Different availability zones

That moment made something very clear:

Dashboard clicking â‰  Real Infrastructure Control.
CLI + IAM + Automation = Real DevOps.

------------------------------------------------------------

Then came the next mindset shift.

Instead of running my app as one single Docker container,
I wanted:

âœ” Auto scaling
âœ” Self-healing
âœ” Load balancing
âœ” Multi-node distribution
âœ” Cost control

Thatâ€™s why I needed a Kubernetes manifest.yml file.

Deployment + Service:

- Deployment â†’ defines replicas (2 pods)
- Rolling update strategy
- Container image
- Port 3000

- Service â†’ type LoadBalancer
  So users can access app through stable endpoint.

Now it wasnâ€™t just â€œrun containerâ€.

It became:

Cluster â†’ Nodes â†’ Pods â†’ Service â†’ LoadBalancer

Thatâ€™s when I truly understood:
Docker runs containers.
Kubernetes manages production systems.





4. Monitoring installed but no data showing ðŸ“‰

Installed Blackbox Exporter. Installed Prometheus. Opened Grafana.

But dashboards were empty.

Why?

Because either: - Exporter was not running OR - Prometheus.yml did not
include its job.

Lesson: Monitoring works like a chain: Exporter â†’ Prometheus â†’ Grafana
If one link is missing, nothing works.

5. Running services in foreground like a beginner ðŸ˜„

Initially I ran:

./prometheus

And the terminal was blocked.

Later I learned to run services properly in background with logs:

./prometheus > prometheus.log 2>&1 &

Lesson: Production services should run in background and logs should be
redirected properly.

6. Almost forgetting to clean resources ðŸ’¸

After finishing practice, I almost left: - EC2 instances - EKS cluster -
Load balancers

Running.

Lesson: Always run:

terraform destroy

And double-check AWS dashboard manually.

Cloud bills do not forgive carelessness.

============================================================ 
FINAL REALIZATION 
============================================================

Every mistake was not a failure. It was a production lesson.

Security groups matter. User context matters. IAM authentication
matters. Monitoring chain matters. Cleanup matters.

These mistakes made me think like a DevOps Engineer, not just someone
installing tools.
